#!/usr/bin/env python3
# ì›Œë‹ í•„í„° (Python 3.14 + Pydantic V1 í˜¸í™˜ì„± ê²½ê³  ìˆ¨ê¹€)
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="langchain_core")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="pydantic")

"""
ISD Agent Benchmark í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

3ì¢… Agent(EduPlanner, Baseline-SolarPro2, ReAct-ISD)ë¥¼
ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ì‹¤í–‰í•˜ê³  ë¹„êµ í‰ê°€í•©ë‹ˆë‹¤.
Upstage Solar Pro2 APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import json
import os
import subprocess
import sys
import threading
import time
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Any, Dict

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    tqdm = None

# Agent ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
_SCRIPT_DIR = Path(__file__).parent
sys.path.insert(0, str(_SCRIPT_DIR / "agents" / "baseline" / "src"))
sys.path.insert(0, str(_SCRIPT_DIR / "agents" / "eduplanner" / "src"))
sys.path.insert(0, str(_SCRIPT_DIR / "agents" / "react-isd" / "src"))
sys.path.insert(0, str(_SCRIPT_DIR / "agents" / "addie-agent" / "src"))
sys.path.insert(0, str(_SCRIPT_DIR / "agents" / "dick-carey-agent" / "src"))
sys.path.insert(0, str(_SCRIPT_DIR / "agents" / "rpisd-agent" / "src"))


class SolarKeyRotator:
    """Solar Pro 3ìš© ë¼ìš´ë“œ ë¡œë¹ˆ API í‚¤ ë¡œí…Œì´í„°"""

    def __init__(self):
        self._lock = threading.Lock()
        self._index = 0

        # ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ ëª©ë¡ êµ¬ì„±
        self._keys = []

        # 1. OpenRouter (upstage/solar-pro-3:free)
        openrouter_key = os.getenv("OPENROUTER_API_KEY")
        if openrouter_key:
            self._keys.append({
                "provider": "openrouter",
                "model": "upstage/solar-pro-3:free",
                "api_key": openrouter_key,
                "base_url": "https://openrouter.ai/api/v1",
            })

        # 2. Upstage Direct (UPSTAGE_API_KEY)
        upstage_key = os.getenv("UPSTAGE_API_KEY")
        if upstage_key:
            self._keys.append({
                "provider": "upstage",
                "model": "solar-pro3",
                "api_key": upstage_key,
                "base_url": "https://api.upstage.ai/v1/solar",
            })

        # 3. Upstage Direct 2 (UPSTAGE_API_KEY2)
        upstage_key2 = os.getenv("UPSTAGE_API_KEY2")
        if upstage_key2:
            self._keys.append({
                "provider": "upstage2",
                "model": "solar-pro3",
                "api_key": upstage_key2,
                "base_url": "https://api.upstage.ai/v1/solar",
            })

        if not self._keys:
            raise ValueError("No API keys found for Solar Pro 3")

        print(f"  [SolarKeyRotator] {len(self._keys)}ê°œ í‚¤ ë¡œë“œë¨: {[k['provider'] for k in self._keys]}")

    def get_next(self) -> dict:
        """ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ ë‹¤ìŒ í‚¤ ë°˜í™˜"""
        with self._lock:
            key_info = self._keys[self._index]
            self._index = (self._index + 1) % len(self._keys)
            return key_info


# ì „ì—­ Solar í‚¤ ë¡œí…Œì´í„° (Solar ëª¨ë¸ ì‚¬ìš© ì‹œì—ë§Œ ì´ˆê¸°í™”)
_solar_key_rotator: Optional[SolarKeyRotator] = None


def get_solar_key_rotator() -> SolarKeyRotator:
    """Solar í‚¤ ë¡œí…Œì´í„° ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _solar_key_rotator
    if _solar_key_rotator is None:
        _solar_key_rotator = SolarKeyRotator()
    return _solar_key_rotator


class BenchmarkProgressLogger:
    """ë²¤ì¹˜ë§ˆí¬ ì§„í–‰ ìƒí™©ì„ ì¹œì ˆí•˜ê²Œ ì¶œë ¥í•˜ëŠ” ë¡œê±°"""

    def __init__(self, total_scenarios: int, total_agents: int, log_file: Optional[Path] = None):
        self.total_scenarios = total_scenarios
        self.total_agents = total_agents
        self.total_tasks = total_scenarios * total_agents  # ì´ ì‘ì—… ìˆ˜

        self.completed_scenarios = 0
        self.completed_tasks = 0
        self.start_time = time.time()
        self.scenario_times = []  # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì†Œìš”ì‹œê°„ ê¸°ë¡

        self.log_file = log_file
        self.lock = threading.Lock()

        # ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”
        if self.log_file:
            self._write_log_header()

    def _write_log_header(self):
        """ë¡œê·¸ íŒŒì¼ í—¤ë” ì‘ì„±"""
        header = f"""
================================================================================
  ISD Agent Benchmark ì‹¤í–‰ ë¡œê·¸
  ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
================================================================================

ì´ ì‹œë‚˜ë¦¬ì˜¤: {self.total_scenarios}ê°œ
ì´ ì—ì´ì „íŠ¸: {self.total_agents}ê°œ
ì´ ì‘ì—… ìˆ˜: {self.total_tasks}ê°œ (ì‹œë‚˜ë¦¬ì˜¤ Ã— ì—ì´ì „íŠ¸)

================================================================================
"""
        if self.log_file:
            with open(self.log_file, 'w', encoding='utf-8') as f:
                f.write(header)
        print(header)

    def _estimate_remaining_time(self) -> str:
        """ë‚¨ì€ ì‹œê°„ ì˜ˆì¸¡"""
        if not self.scenario_times:
            return "ê³„ì‚° ì¤‘..."

        avg_time = sum(self.scenario_times) / len(self.scenario_times)
        remaining_scenarios = self.total_scenarios - self.completed_scenarios
        remaining_seconds = avg_time * remaining_scenarios

        if remaining_seconds < 60:
            return f"{int(remaining_seconds)}ì´ˆ"
        elif remaining_seconds < 3600:
            return f"{int(remaining_seconds / 60)}ë¶„ {int(remaining_seconds % 60)}ì´ˆ"
        else:
            hours = int(remaining_seconds / 3600)
            minutes = int((remaining_seconds % 3600) / 60)
            return f"{hours}ì‹œê°„ {minutes}ë¶„"

    def _estimate_completion_time(self) -> str:
        """ì˜ˆìƒ ì™„ë£Œ ì‹œê°„"""
        if not self.scenario_times:
            return "ê³„ì‚° ì¤‘..."

        avg_time = sum(self.scenario_times) / len(self.scenario_times)
        remaining_scenarios = self.total_scenarios - self.completed_scenarios
        remaining_seconds = avg_time * remaining_scenarios

        completion_time = datetime.now() + timedelta(seconds=remaining_seconds)
        return completion_time.strftime('%H:%M:%S')

    def _get_progress_bar(self, current: int, total: int, width: int = 30) -> str:
        """ì§„í–‰ ë°” ìƒì„±"""
        if total == 0:
            return "â–‘" * width

        filled = int(width * current / total)
        empty = width - filled
        percentage = (current / total) * 100

        return f"{'â–ˆ' * filled}{'â–‘' * empty} {percentage:5.1f}%"

    def _format_elapsed_time(self) -> str:
        """ê²½ê³¼ ì‹œê°„ í¬ë§·"""
        elapsed = time.time() - self.start_time
        if elapsed < 60:
            return f"{int(elapsed)}ì´ˆ"
        elif elapsed < 3600:
            return f"{int(elapsed / 60)}ë¶„ {int(elapsed % 60)}ì´ˆ"
        else:
            hours = int(elapsed / 3600)
            minutes = int((elapsed % 3600) / 60)
            return f"{hours}ì‹œê°„ {minutes}ë¶„"

    def log_scenario_start(self, scenario_id: str, scenario_index: int):
        """ì‹œë‚˜ë¦¬ì˜¤ ì‹œì‘ ë¡œê·¸"""
        with self.lock:
            progress_bar = self._get_progress_bar(scenario_index, self.total_scenarios)
            remaining = self._estimate_remaining_time()
            completion = self._estimate_completion_time()

            log_msg = f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š ì§„í–‰ í˜„í™©: [{progress_bar}]
â”‚
â”‚ ğŸ”„ í˜„ì¬ ì‘ì—…: [{scenario_index + 1}/{self.total_scenarios}] {scenario_id}
â”‚ â±ï¸  ê²½ê³¼ ì‹œê°„: {self._format_elapsed_time()}
â”‚ â³ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining}
â”‚ ğŸ ì˜ˆìƒ ì™„ë£Œ ì‹œê°„: {completion}
â”‚
â”‚ ë‚¨ì€ ì‹œë‚˜ë¦¬ì˜¤: {self.total_scenarios - scenario_index - 1}ê°œ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
            print(log_msg)
            if self.log_file:
                with open(self.log_file, 'a', encoding='utf-8') as f:
                    f.write(f"\n[{datetime.now().strftime('%H:%M:%S')}] ì‹œì‘: {scenario_id} ({scenario_index + 1}/{self.total_scenarios})\n")

    def log_agent_progress(self, scenario_id: str, agent_id: str, agent_index: int,
                           total_agents: int, status: str):
        """ì—ì´ì „íŠ¸ ì§„í–‰ ë¡œê·¸"""
        with self.lock:
            status_icon = "âœ…" if status == "success" else "âŒ" if status == "failed" else "ğŸ”„"
            log_msg = f"    {status_icon} [{agent_index + 1}/{total_agents}] {agent_id}: {status}"
            print(log_msg)

            if self.log_file:
                with open(self.log_file, 'a', encoding='utf-8') as f:
                    f.write(f"  - {agent_id}: {status}\n")

    def log_scenario_complete(self, scenario_id: str, elapsed_seconds: float, success_count: int):
        """ì‹œë‚˜ë¦¬ì˜¤ ì™„ë£Œ ë¡œê·¸"""
        with self.lock:
            self.completed_scenarios += 1
            self.scenario_times.append(elapsed_seconds)

            log_msg = f"""
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    âœ… ì™„ë£Œ: {scenario_id}
    â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_seconds:.1f}ì´ˆ
    ğŸ“ˆ ì„±ê³µí•œ ì—ì´ì „íŠ¸: {success_count}/{self.total_agents}
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
            print(log_msg)

            if self.log_file:
                with open(self.log_file, 'a', encoding='utf-8') as f:
                    f.write(f"[{datetime.now().strftime('%H:%M:%S')}] ì™„ë£Œ: {scenario_id} ({elapsed_seconds:.1f}ì´ˆ, ì„±ê³µ: {success_count})\n")

    def log_final_summary(self, results: dict):
        """ìµœì¢… ìš”ì•½ ë¡œê·¸"""
        total_elapsed = time.time() - self.start_time

        # í†µê³„ ê³„ì‚°
        total_success = 0
        total_failed = 0
        for variant_results in results.get("scenarios", {}).values():
            for scenario_result in variant_results.values():
                for agent_result in scenario_result.get("agents", {}).values():
                    if agent_result.get("success"):
                        total_success += 1
                    else:
                        total_failed += 1

        summary = f"""

================================================================================
  ğŸ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!
================================================================================

ğŸ“Š ìµœì¢… ê²°ê³¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ì´ ì‹œë‚˜ë¦¬ì˜¤:     {self.completed_scenarios}ê°œ
  ì´ ì‘ì—… ìˆ˜:      {total_success + total_failed}ê°œ
  ì„±ê³µ:           {total_success}ê°œ âœ…
  ì‹¤íŒ¨:           {total_failed}ê°œ âŒ
  ì„±ê³µë¥ :         {(total_success / (total_success + total_failed) * 100) if (total_success + total_failed) > 0 else 0:.1f}%

â±ï¸  ì´ ì†Œìš” ì‹œê°„: {self._format_elapsed_time()}
ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {results.get('output_dir', 'N/A')}

================================================================================
"""
        print(summary)

        if self.log_file:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(summary)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv(Path(__file__).parent / ".env")
except ImportError:
    pass  # dotenvê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€


# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ (isd-agent-bench-en ë””ë ‰í† ë¦¬)
PROJECT_ROOT = Path(__file__).parent
SCENARIOS_DIR = PROJECT_ROOT / "scenarios"
RESULTS_DIR = PROJECT_ROOT / "results"
VENV_BIN = PROJECT_ROOT / ".venv" / "bin"

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (Agent ì‹¤í–‰ìš©)
def get_env_with_venv():
    """venv bin ê²½ë¡œê°€ í¬í•¨ëœ í™˜ê²½ë³€ìˆ˜ ë°˜í™˜"""
    env = os.environ.copy()
    venv_path = str(VENV_BIN)
    current_path = env.get('PATH', '')
    env['PATH'] = f"{venv_path}:{current_path}"
    return env


def get_all_scenarios(
    use_stratified_sampling: bool = False,
    n_samples: int | None = None,
    sampling_strategy: str = "oversample",
    dataset: str | None = None,
) -> dict[str, list[Path]]:
    """
    ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ì„ variantë³„ë¡œ ìˆ˜ì§‘ (IDLD ë°ì´í„°ì…‹ êµ¬ì¡°)

    ë°ì´í„°ì…‹ êµ¬ì¡° ê´€ê³„:
    - dataset=None (ê¸°ë³¸): ê¸°ì¡´ variant ë””ë ‰í† ë¦¬ ì‚¬ìš© (idld_aligned, context_variant)
    - dataset="train": í•™ìŠµìš© ë°ì´í„°ì…‹ (scenarios/train/) - idld_aligned + context_variantì—ì„œ ë¶„ë¦¬
    - dataset="test": í‰ê°€ìš© ë°ì´í„°ì…‹ (scenarios/test/) - Hold-out í‰ê°€ìš©, 5% ë¹„ìœ¨

    Args:
        use_stratified_sampling: ì¸µí™” ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€ (ë¶ˆê· í˜• ì¶• ë³´ì •)
        n_samples: ìƒ˜í”Œë§ ì‹œ ì¶”ì¶œí•  ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        sampling_strategy: ìƒ˜í”Œë§ ì „ëµ ("oversample", "undersample", "proportional")
        dataset: ë°ì´í„°ì…‹ ì„ íƒ ("train", "test", None=variant ëª¨ë“œ)

    Returns:
        variant/datasetë³„ ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
    """
    # dataset ëª¨ë“œ: train/test ë””ë ‰í† ë¦¬ì—ì„œ ì§ì ‘ ë¡œë“œ
    if dataset in ("train", "test"):
        dataset_dir = SCENARIOS_DIR / dataset
        if not dataset_dir.exists():
            print(f"[ê²½ê³ ] {dataset} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_dir}")
            return {dataset: []}

        scenario_files = sorted(dataset_dir.glob("*.json"))
        print(f"  [{dataset.upper()}] {len(scenario_files)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œë¨")

        # ì¸µí™” ìƒ˜í”Œë§ ì§€ì› (train ë°ì´í„°ì…‹ì—ì„œë§Œ ì˜ë¯¸ ìˆìŒ)
        if use_stratified_sampling and n_samples and dataset == "train":
            try:
                from scenarios.sampling_strategy import StratifiedScenarioSampler
                sampler = StratifiedScenarioSampler(scenarios_dir=dataset_dir)
                sampled = sampler.sample_with_paths(n_samples, strategy=sampling_strategy)
                scenario_files = [path for path, _ in sampled]
                print(f"  [ì¸µí™” ìƒ˜í”Œë§] {dataset}: {len(scenario_files)}ê°œ ì„ íƒ (ì „ëµ: {sampling_strategy})")
            except ImportError:
                pass  # ìƒ˜í”Œë§ ëª¨ë“ˆ ì—†ìœ¼ë©´ ì „ì²´ ì‚¬ìš©

        return {dataset: scenario_files}

    # ê¸°ì¡´ variant ëª¨ë“œ: idld_aligned, context_variant ë””ë ‰í† ë¦¬ ì‚¬ìš©
    scenarios = {"idld_aligned": [], "context_variant": []}

    for variant in scenarios.keys():
        variant_dir = SCENARIOS_DIR / variant
        if variant_dir.exists():
            if use_stratified_sampling and variant == "idld_aligned" and n_samples:
                # ì¸µí™” ìƒ˜í”Œë§ ì ìš© (ë¶ˆê· í˜• ì¶• ë³´ì •)
                try:
                    from scenarios.sampling_strategy import StratifiedScenarioSampler
                    sampler = StratifiedScenarioSampler(scenarios_dir=variant_dir)
                    sampled = sampler.sample_with_paths(n_samples, strategy=sampling_strategy)
                    scenarios[variant] = [path for path, _ in sampled]
                    print(f"  [ì¸µí™” ìƒ˜í”Œë§] {variant}: {len(scenarios[variant])}ê°œ ì„ íƒ (ì „ëµ: {sampling_strategy})")
                except ImportError:
                    # ìƒ˜í”Œë§ ëª¨ë“ˆ ì—†ìœ¼ë©´ ê¸°ë³¸ ë™ì‘
                    for scenario_file in sorted(variant_dir.glob("*.json")):
                        scenarios[variant].append(scenario_file)
            else:
                for scenario_file in sorted(variant_dir.glob("*.json")):
                    scenarios[variant].append(scenario_file)

    return scenarios


def install_agents() -> bool:
    """ëª¨ë“  Agent íŒ¨í‚¤ì§€ ì„¤ì¹˜"""
    print("\n" + "=" * 60)
    print("Agent íŒ¨í‚¤ì§€ ì„¤ì¹˜")
    print("=" * 60)

    agents = [
        PROJECT_ROOT / "agents" / "eduplanner",
        PROJECT_ROOT / "agents" / "baseline",
        PROJECT_ROOT / "agents" / "react-isd",
        PROJECT_ROOT / "agents" / "addie-agent",
        PROJECT_ROOT / "agents" / "dick-carey-agent",
        PROJECT_ROOT / "agents" / "rpisd-agent",
        PROJECT_ROOT / "evaluator",
    ]

    for agent_path in agents:
        print(f"\nì„¤ì¹˜ ì¤‘: {agent_path.name}")
        result = subprocess.run(
            ["pip", "install", "-e", str(agent_path)],
            capture_output=True,
            text=True,
        )
        if result.returncode != 0:
            print(f"  ì˜¤ë¥˜: {result.stderr}")
            return False
        print(f"  ì™„ë£Œ")

    return True


def check_agents_installed() -> dict[str, bool]:
    """Agent ëª¨ë“ˆ import ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
    agents = {
        "eduplanner": False,
        "baseline": False,
        "react-isd": False,
        "addie-agent": False,
        "dick-carey-agent": False,
        "rpisd-agent": False,
    }

    # ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸
    try:
        from baseline.generator import BaselineGenerator
        agents["baseline"] = True
    except ImportError:
        pass

    try:
        from eduplanner.agents import EduPlannerAgent
        agents["eduplanner"] = True
    except ImportError:
        pass

    try:
        from react_isd.agent import ReActISDAgent
        agents["react-isd"] = True
    except ImportError:
        pass

    try:
        from addie_agent.agent import ADDIEAgent
        agents["addie-agent"] = True
    except ImportError:
        pass

    try:
        from dick_carey_agent.agent import DickCareyAgent
        agents["dick-carey-agent"] = True
    except ImportError:
        pass

    try:
        from rpisd_agent.agent import RPISDAgent
        agents["rpisd-agent"] = True
    except ImportError:
        pass

    return agents


def _get_model_config() -> tuple[str, str, Optional[str]]:
    """ëª¨ë¸ ì„¤ì • ë°˜í™˜. Solar Pro 3ì´ë©´ í‚¤ ë¡œí…Œì´ì…˜ ì ìš©.

    Returns:
        (provider, model, api_key) - api_keyëŠ” Solar ë¡œí…Œì´ì…˜ ì‹œì—ë§Œ ì„¤ì •
    """
    provider = os.getenv("MODEL_PROVIDER", "openrouter")
    model = os.getenv("MODEL_NAME", "anthropic/claude-opus-4.5")

    # Solar Pro 3 ëª¨ë¸ ê°ì§€ (í‚¤ ë¡œí…Œì´ì…˜ ì ìš©)
    is_solar = "solar" in model.lower()
    if is_solar:
        rotator = get_solar_key_rotator()
        key_info = rotator.get_next()
        # í™˜ê²½ë³€ìˆ˜ ì„ì‹œ ì„¤ì • (ì—ì´ì „íŠ¸ë“¤ì´ ì½ì„ ìˆ˜ ìˆë„ë¡)
        os.environ["_ROTATED_API_KEY"] = key_info["api_key"]
        os.environ["_ROTATED_BASE_URL"] = key_info["base_url"]
        return key_info["provider"], key_info["model"], key_info["api_key"]

    return provider, model, None


def _get_agent_runner(agent_id: str):
    """ì—ì´ì „íŠ¸ IDì— í•´ë‹¹í•˜ëŠ” ì‹¤í–‰ í•¨ìˆ˜ ë°˜í™˜ (ëª¨ë“ˆ ê¸°ë°˜)"""

    if agent_id == "baseline":
        from baseline.generator import BaselineGenerator
        def run_baseline(scenario: dict) -> dict:
            provider, model, api_key = _get_model_config()
            gen = BaselineGenerator(model=model, provider=provider, api_key=api_key)
            return gen.generate(scenario)
        return run_baseline

    elif agent_id == "eduplanner":
        from eduplanner.agents import EduPlannerAgent
        from eduplanner.agents.base import AgentConfig
        from eduplanner.models.schemas import ScenarioInput
        def run_eduplanner(scenario: dict) -> dict:
            provider, model, api_key = _get_model_config()
            config = AgentConfig(model=model, provider=provider)
            agent = EduPlannerAgent(config=config, max_iterations=3, target_score=90.0)
            scenario_input = ScenarioInput(**scenario)
            result = agent.run(scenario_input)
            return {
                "addie_output": result.addie_output.to_standard_dict(),
                "trajectory": result.trajectory.model_dump(),
                "metadata": result.metadata.model_dump(),
            }
        return run_eduplanner

    elif agent_id == "react-isd":
        from react_isd.agent import ReActISDAgent
        def run_react(scenario: dict) -> dict:
            provider, model, api_key = _get_model_config()
            agent = ReActISDAgent(model=model, provider=provider, api_key=api_key)
            return agent.run(scenario)
        return run_react

    elif agent_id == "addie-agent":
        from addie_agent.agent import ADDIEAgent
        def run_addie(scenario: dict) -> dict:
            provider, model, api_key = _get_model_config()
            agent = ADDIEAgent(model=model)
            return agent.run(scenario)
        return run_addie

    elif agent_id == "dick-carey-agent":
        from dick_carey_agent.agent import DickCareyAgent
        def run_dickcarey(scenario: dict) -> dict:
            provider, model, api_key = _get_model_config()
            agent = DickCareyAgent(model=model)
            return agent.run(scenario)
        return run_dickcarey

    elif agent_id == "rpisd-agent":
        from rpisd_agent.agent import RPISDAgent
        def run_rpisd(scenario: dict) -> dict:
            provider, model, api_key = _get_model_config()
            agent = RPISDAgent(model=model)
            return agent.run(scenario)
        return run_rpisd

    else:
        raise ValueError(f"Unknown agent: {agent_id}")


def _run_agent_task(
    agent_id: str,
    scenario_path: Path,
    output_dir: Path,
    semaphore: Optional[threading.Semaphore] = None,
) -> tuple[str, dict]:
    """ê°œë³„ Agent ì‹¤í–‰ íƒœìŠ¤í¬ (ëª¨ë“ˆ ê¸°ë°˜, ë³‘ë ¬ ì‹¤í–‰ìš©)"""
    if semaphore:
        semaphore.acquire()

    try:
        output_path = output_dir / f"{agent_id}_output.json"
        trajectory_path = output_dir / f"{agent_id}_trajectory.json"
        log_path = output_dir / f"{agent_id}_log.txt"

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir.mkdir(parents=True, exist_ok=True)

        try:
            # ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ
            with open(scenario_path, "r", encoding="utf-8") as f:
                scenario = json.load(f)

            # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ëª¨ë“ˆ ê¸°ë°˜)
            start_time = time.time()
            runner = _get_agent_runner(agent_id)
            result = runner(scenario)
            elapsed = time.time() - start_time

            # ADDIE ì¶œë ¥ ì €ì¥
            addie_output = result.get("addie_output", result)
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(addie_output, f, ensure_ascii=False, indent=2, default=str)

            # Trajectory ì €ì¥
            trajectory_data = {
                "scenario_id": scenario.get("scenario_id", "unknown"),
                "agent_id": agent_id,
                "timestamp": datetime.now().isoformat(),
                "trajectory": result.get("trajectory", {}),
                "metadata": result.get("metadata", {"execution_time_seconds": elapsed}),
            }
            with open(trajectory_path, "w", encoding="utf-8") as f:
                json.dump(trajectory_data, f, ensure_ascii=False, indent=2, default=str)

            # ë¡œê·¸ ì €ì¥
            with open(log_path, "w", encoding="utf-8") as f:
                f.write(f"=== {agent_id} ì‹¤í–‰ ë¡œê·¸ ===\n")
                f.write(f"Scenario: {scenario_path}\n")
                f.write(f"Elapsed: {elapsed:.2f}s\n")
                f.write(f"Status: SUCCESS\n")

            return agent_id, {
                "success": True,
                "output_path": str(output_path),
                "trajectory_path": str(trajectory_path),
                "log_path": str(log_path),
            }

        except Exception as e:
            error_msg = str(e)
            tb = traceback.format_exc()

            with open(log_path, "w", encoding="utf-8") as f:
                f.write(f"=== {agent_id} ì‹¤í–‰ ë¡œê·¸ ===\n")
                f.write(f"Scenario: {scenario_path}\n")
                f.write(f"Status: FAILED\n")
                f.write(f"Error: {error_msg}\n\n")
                f.write("=== Traceback ===\n")
                f.write(tb)

            return agent_id, {
                "success": False,
                "error": error_msg[:500],
                "log_path": str(log_path),
            }

    finally:
        if semaphore:
            semaphore.release()


def run_single_benchmark(
    scenario_path: Path,
    output_dir: Path,
    agents: Optional[list[str]] = None,
    verbose: bool = False,
    parallel: bool = False,
    max_workers: int = 3,
    multi_judge: bool = True,
) -> dict:
    """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

    Args:
        scenario_path: ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        agents: ì‹¤í–‰í•  Agent ëª©ë¡
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        parallel: Agent ë³‘ë ¬ ì‹¤í–‰ ì—¬ë¶€
        max_workers: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
    """
    agents = agents or ["eduplanner", "baseline", "react-isd", "addie-agent", "dick-carey-agent", "rpisd-agent"]

    print(f"\nì‹œë‚˜ë¦¬ì˜¤: {scenario_path.name}")
    print("-" * 40)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)

    results = {
        "scenario": scenario_path.name,
        "agents": {},
        "timestamp": datetime.now().isoformat(),
    }

    if parallel and len(agents) > 1:
        # ë³‘ë ¬ ì‹¤í–‰: ThreadPoolExecutor + Semaphore
        effective_workers = max_workers
        print(f"  [ë³‘ë ¬ ëª¨ë“œ] {len(agents)}ê°œ Agent ë™ì‹œ ì‹¤í–‰ (max_workers={effective_workers})")
        semaphore = threading.Semaphore(effective_workers)

        with ThreadPoolExecutor(max_workers=len(agents)) as executor:
            futures = {}
            for idx, agent_id in enumerate(agents):
                future = executor.submit(
                    _run_agent_task, agent_id, scenario_path, output_dir, semaphore
                )
                futures[future] = agent_id
                # Rate limit ëŒ€ì‘: Agent ì œì¶œ ê°„ ë”œë ˆì´
                time.sleep(float(os.getenv("BENCHMARK_DELAY", "2.0")))

            for future in as_completed(futures):
                agent_id = futures[future]
                try:
                    _, agent_result = future.result()
                    status = "ì™„ë£Œ" if agent_result["success"] else "ì‹¤íŒ¨"
                    print(f"  {agent_id}: {status}")
                    results["agents"][agent_id] = agent_result

                    if verbose and not agent_result["success"]:
                        print(f"    stderr: {agent_result.get('stderr', '')[:200]}")
                except Exception as e:
                    print(f"  {agent_id}: ì˜ˆì™¸ ë°œìƒ")
                    results["agents"][agent_id] = {
                        "success": False,
                        "error": str(e),
                    }
    else:
        # ìˆœì°¨ ì‹¤í–‰ (ê¸°ì¡´ ë°©ì‹)
        for agent_id in agents:
            print(f"  {agent_id} ì‹¤í–‰ ì¤‘...", end=" ", flush=True)
            _, agent_result = _run_agent_task(agent_id, scenario_path, output_dir)

            status = "ì™„ë£Œ" if agent_result["success"] else "ì‹¤íŒ¨"
            print(status)
            results["agents"][agent_id] = agent_result

            if verbose and not agent_result["success"]:
                print(f"    stderr: {agent_result.get('stderr', '')[:200]}")

            # Rate limit ëŒ€ì‘: Agent ê°„ ë”œë ˆì´
            time.sleep(float(os.getenv("BENCHMARK_DELAY", "2.0")))

    # í‰ê°€ ì‹¤í–‰
    successful_agents = [a for a, r in results["agents"].items() if r.get("success")]

    if len(successful_agents) >= 2:
        print(f"  Evaluating...", end=" ", flush=True)

        cmd = [
            "isd-evaluator",
            "compare",
            "--scenario", str(scenario_path),
            "--output-dir", str(output_dir),
            "--agents", ",".join(successful_agents),
        ]

        # Use multi-judge (5 LLMs) or single-judge
        if multi_judge:
            cmd.append("--multi-judge")
        else:
            cmd.extend(["--single-judge", "--use-llm"])

        if verbose:
            cmd.append("--verbose")

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            env=get_env_with_venv(),
            # íƒ€ì„ì•„ì›ƒ ì—†ìŒ - LLM í‰ê°€ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
        )

        if result.returncode == 0:
            print("ì™„ë£Œ")

            # í‰ê°€ ê²°ê³¼ ë¡œë“œ
            report_path = output_dir / "comparison_report.json"
            if report_path.exists():
                with open(report_path, "r", encoding="utf-8") as f:
                    results["evaluation"] = json.load(f)
        else:
            print("ì‹¤íŒ¨")
            results["evaluation_error"] = result.stderr[:500] if result.stderr else "Unknown error"
    else:
        print(f"  í‰ê°€ ìƒëµ (ì„±ê³µí•œ Agent: {len(successful_agents)}ê°œ)")
        results["evaluation_skipped"] = True

    return results


def _run_scenario_task(
    scenario_path: Path,
    output_dir: Path,
    agents: Optional[list[str]],
    verbose: bool,
    parallel: bool,
    max_workers: int,
    multi_judge: bool = True,
    semaphore: Optional[threading.Semaphore] = None,
) -> tuple[str, dict]:
    """Scenario execution task (for scenario-level parallelization)"""
    if semaphore:
        semaphore.acquire()

    try:
        scenario_id = scenario_path.stem
        result = run_single_benchmark(
            scenario_path=scenario_path,
            output_dir=output_dir,
            agents=agents,
            verbose=verbose,
            parallel=parallel,
            max_workers=max_workers,
            multi_judge=multi_judge,
        )
        return scenario_id, result
    finally:
        if semaphore:
            semaphore.release()


def run_full_benchmark(
    variants: Optional[list[str]] = None,
    agents: Optional[list[str]] = None,
    verbose: bool = False,
    parallel: bool = True,
    max_workers: int = 6,
    scenario_parallel: bool = True,
    scenario_max_workers: int = 8,
    dataset: Optional[str] = None,
    multi_judge: bool = True,
) -> dict:
    """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (IDLD ë°ì´í„°ì…‹ êµ¬ì¡°)

    Args:
        variants: ì‹¤í–‰í•  variant ëª©ë¡ (dataset=Noneì¼ ë•Œë§Œ ì‚¬ìš©)
        agents: ì‹¤í–‰í•  Agent ëª©ë¡
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        parallel: Agent ë ˆë²¨ ë³‘ë ¬ ì‹¤í–‰ ì—¬ë¶€ (ê¸°ë³¸: True)
        max_workers: Agent ë™ì‹œ ì‹¤í–‰ ìˆ˜ (ê¸°ë³¸: 6)
        scenario_parallel: ì‹œë‚˜ë¦¬ì˜¤ ë ˆë²¨ ë³‘ë ¬ ì‹¤í–‰ ì—¬ë¶€ (ê¸°ë³¸: True)
        scenario_max_workers: ì‹œë‚˜ë¦¬ì˜¤ ë™ì‹œ ì‹¤í–‰ ìˆ˜ (ê¸°ë³¸: 8)
        dataset: ë°ì´í„°ì…‹ ì„ íƒ ("train", "test", None=variant ëª¨ë“œ)
    """
    # dataset ëª¨ë“œì¼ ë•ŒëŠ” variants ë¬´ì‹œ
    if dataset:
        variants = [dataset]  # datasetì„ variantì²˜ëŸ¼ ì²˜ë¦¬
    else:
        variants = variants or ["idld_aligned", "context_variant"]

    agents = agents or ["eduplanner", "baseline", "react-isd", "addie-agent", "dick-carey-agent", "rpisd-agent"]

    # ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜ì§‘ (ë¨¼ì € ìˆ˜ì§‘í•˜ì—¬ ì´ ê°œìˆ˜ íŒŒì•…)
    all_scenarios = get_all_scenarios(dataset=dataset)
    total_scenarios = sum(len(s) for s in all_scenarios.values())

    # íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ëª¨ë¸ ì´ë¦„ì—ì„œ ë””ë ‰í† ë¦¬ìš© safe name ìƒì„± (ì˜ˆ: anthropic/claude-opus-4.5 -> claude-opus-4.5)
    model_name = os.getenv("MODEL_NAME", "default")
    model_safe_name = model_name.split("/")[-1].replace(":", "-")  # Remove provider prefix and replace colons

    # dataset ëª¨ë“œì¼ ë•ŒëŠ” ë””ë ‰í† ë¦¬ ì´ë¦„ì— ë°˜ì˜ (ì˜ˆ: test_benchmark_claude-opus-4.5_20260122_...)
    if dataset:
        run_dir = RESULTS_DIR / f"{dataset}_benchmark_{model_safe_name}_{timestamp}"
    else:
        run_dir = RESULTS_DIR / f"benchmark_{model_safe_name}_{timestamp}"
    run_dir.mkdir(parents=True, exist_ok=True)

    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    log_file = run_dir / "benchmark_progress.log"

    # ë¡œê±° ì´ˆê¸°í™”
    logger = BenchmarkProgressLogger(
        total_scenarios=total_scenarios,
        total_agents=len(agents),
        log_file=log_file
    )

    print(f"\n{'=' * 80}")
    print(f"  ğŸš€ ISD Agent Benchmark ì‹¤í–‰")
    print(f"{'=' * 80}")
    print(f"  ğŸ¤– Model: {model_name} (provider: {os.getenv('MODEL_PROVIDER', 'openrouter')})")

    if dataset:
        print(f"  ğŸ“‚ ë°ì´í„°ì…‹ ëª¨ë“œ: {dataset.upper()}")
    else:
        print(f"  ğŸ“‚ Variant ëª¨ë“œ: {', '.join(variants)}")

    print(f"  Agents ({len(agents)}): {', '.join(agents)}")
    print(f"  Total scenarios: {total_scenarios}")
    print(f"  Parallel mode: {scenario_max_workers} scenarios x {max_workers} agents concurrently")
    if multi_judge:
        print(f"  Evaluation: Multi-Judge (2 LLMs, ê²½ëŸ‰/ë¹ ë¦„)")
        print(f"    - openai/gpt-4o-mini (OPENROUTER_API_KEY)")
        print(f"    - google/gemini-2.5-flash-lite (OPENROUTER_API_KEY)")
    else:
        print(f"  Evaluation: Single-Judge")
    print(f"  Output: {run_dir}")
    print(f"  Log file: {log_file}")
    print(f"{'=' * 80}\n")

    results = {
        "timestamp": timestamp,
        "output_dir": str(run_dir),
        "model": {
            "name": model_name,
            "provider": os.getenv("MODEL_PROVIDER", "openrouter"),
        },
        "config": {
            "variants": variants,
            "dataset": dataset,
            "agents": agents,
            "parallel": parallel,
            "max_workers": max_workers,
            "scenario_parallel": scenario_parallel,
            "scenario_max_workers": scenario_max_workers,
            "multi_judge": multi_judge,
            "judge_models": [
                "openai/gpt-4o-mini",
                "google/gemini-2.5-flash-lite",
            ] if multi_judge else None,
        },
        "scenarios": {},
    }

    scenario_index = 0

    # ê° variantë³„ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
    for variant in variants:
        scenarios = all_scenarios.get(variant, [])
        if not scenarios:
            print(f"\n[{variant}] ì‹œë‚˜ë¦¬ì˜¤ ì—†ìŒ")
            continue

        print(f"\n{'â”€' * 80}")
        print(f"  ğŸ“ [{variant.upper()}] {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ì‹œì‘")
        print(f"{'â”€' * 80}")

        results["scenarios"][variant] = {}

        # tqdm ì§„í–‰ë¥  í‘œì‹œ ì„¤ì •
        scenario_iter = scenarios
        if TQDM_AVAILABLE:
            scenario_iter = tqdm(
                scenarios,
                desc=f"[{variant}]",
                unit="scenario",
                leave=True,
                ncols=100,
            )

        if scenario_parallel and len(scenarios) > 1:
            # ì‹œë‚˜ë¦¬ì˜¤ ë ˆë²¨ ë³‘ë ¬ ì‹¤í–‰ (rate limit ëŒ€ì‘: ì›Œì»¤ ìˆ˜ ì œí•œ)
            effective_workers = scenario_max_workers
            semaphore = threading.Semaphore(effective_workers)
            completed_count = 0
            completed_lock = threading.Lock()
            pbar = scenario_iter if TQDM_AVAILABLE else None

            def run_with_logging(scenario_path, scenario_output_dir, idx):
                nonlocal completed_count
                scenario_id = scenario_path.stem
                start_time = time.time()

                # Rate limit ëŒ€ì‘: ì‹œì‘ ì „ ë”œë ˆì´
                time.sleep(float(os.getenv("BENCHMARK_DELAY", "2.0")))

                logger.log_scenario_start(scenario_id, idx)

                result = run_single_benchmark(
                    scenario_path=scenario_path,
                    output_dir=scenario_output_dir,
                    agents=agents,
                    verbose=verbose,
                    parallel=parallel,
                    max_workers=max_workers,
                    multi_judge=multi_judge,
                )

                elapsed = time.time() - start_time
                success_count = sum(1 for r in result.get("agents", {}).values() if r.get("success"))
                logger.log_scenario_complete(scenario_id, elapsed, success_count)

                with completed_lock:
                    completed_count += 1
                    if pbar:
                        pbar.update(1)

                return scenario_id, result

            with ThreadPoolExecutor(max_workers=effective_workers) as executor:
                futures = {}
                for idx, scenario_path in enumerate(scenarios):
                    scenario_output_dir = run_dir / variant / scenario_path.stem

                    future = executor.submit(
                        run_with_logging,
                        scenario_path,
                        scenario_output_dir,
                        scenario_index + idx,
                    )
                    futures[future] = scenario_path.stem
                    # Rate limit ëŒ€ì‘: ì œì¶œ ê°„ ë”œë ˆì´
                    time.sleep(float(os.getenv("BENCHMARK_DELAY", "2.0")))

                for future in as_completed(futures):
                    scenario_id = futures[future]
                    try:
                        _, scenario_result = future.result()
                        results["scenarios"][variant][scenario_id] = scenario_result
                    except Exception as e:
                        print(f"  âŒ {scenario_id}: ì˜ˆì™¸ ë°œìƒ - {e}")
                        results["scenarios"][variant][scenario_id] = {
                            "scenario": scenario_id,
                            "error": str(e),
                        }

            if pbar:
                pbar.close()
            scenario_index += len(scenarios)
        else:
            # Sequential execution with tqdm
            for idx, scenario_path in enumerate(scenario_iter):
                scenario_id = scenario_path.stem
                scenario_output_dir = run_dir / variant / scenario_id
                start_time = time.time()

                logger.log_scenario_start(scenario_id, scenario_index + idx)

                scenario_result = run_single_benchmark(
                    scenario_path=scenario_path,
                    output_dir=scenario_output_dir,
                    agents=agents,
                    verbose=verbose,
                    parallel=parallel,
                    max_workers=max_workers,
                    multi_judge=multi_judge,
                )

                elapsed = time.time() - start_time
                success_count = sum(1 for r in scenario_result.get("agents", {}).values() if r.get("success"))
                logger.log_scenario_complete(scenario_id, elapsed, success_count)

                results["scenarios"][variant][scenario_id] = scenario_result

                # Rate limit ëŒ€ì‘: ì‹œë‚˜ë¦¬ì˜¤ ê°„ ë”œë ˆì´
                time.sleep(float(os.getenv("BENCHMARK_DELAY", "2.0")))

            scenario_index += len(scenarios)

    # ì „ì²´ ê²°ê³¼ ì €ì¥
    summary_path = run_dir / "benchmark_summary.json"
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)

    # ìµœì¢… ìš”ì•½ ë¡œê·¸
    logger.log_final_summary(results)

    return results


def generate_summary_report(results: dict, output_path: Path) -> None:
    """ì „ì²´ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
    lines = [
        "# ISD Agent Benchmark ê²°ê³¼ ìš”ì•½",
        "",
        f"ì‹¤í–‰ ì‹œê°„: {results['timestamp']}",
        "",
        "## ì„¤ì •",
        f"- Variant: {', '.join(results['config']['variants'])}",
        f"- Agent: {', '.join(results['config']['agents'])}",
        "",
        "## ê²°ê³¼ ìš”ì•½",
        "",
    ]

    # ì§‘ê³„
    total_scenarios = 0
    agent_stats = {}

    for variant, scenarios in results.get("scenarios", {}).items():
        lines.append(f"### {variant.upper()}")
        lines.append("")

        for scenario_id, scenario_result in scenarios.items():
            total_scenarios += 1
            lines.append(f"#### {scenario_id}")
            lines.append("")

            for agent_id, agent_result in scenario_result.get("agents", {}).items():
                if agent_id not in agent_stats:
                    agent_stats[agent_id] = {"success": 0, "failed": 0}

                if agent_result.get("success"):
                    agent_stats[agent_id]["success"] += 1
                    status = "ì„±ê³µ"
                else:
                    agent_stats[agent_id]["failed"] += 1
                    status = f"ì‹¤íŒ¨: {agent_result.get('error', 'Unknown')[:50]}"

                lines.append(f"- {agent_id}: {status}")

            # í‰ê°€ ê²°ê³¼
            if "evaluation" in scenario_result:
                eval_data = scenario_result["evaluation"]
                lines.append("")
                lines.append("**í‰ê°€ ì ìˆ˜:**")

                # Comparison rankingsì—ì„œ ì ìˆ˜ ì¶”ì¶œ (ranking ìˆœì„œëŒ€ë¡œ í‘œì‹œ)
                comparison = eval_data.get("comparison", {})
                rankings = comparison.get("rankings", [])
                
                # ìˆœìœ„ìˆœ ì •ë ¬ ë³´ì¥
                rankings.sort(key=lambda x: x.get("rank", 999))
                
                for rank_info in rankings:
                    agent_id = rank_info.get("agent_id", "unknown")
                    total = rank_info.get("total_score", 0)
                    process = rank_info.get("process_score")
                    
                    score_str = f"{total:.1f}/100"
                    if process is not None:
                         score_str += f" (ê³¼ì •: {process:.1f})"
                         
                    lines.append(f"- {agent_id}: {score_str}")

                # ì—ëŸ¬ ë“±ìœ¼ë¡œ rankingsê°€ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„
                if not rankings and "agents" in eval_data:
                     # ê¸°ì¡´ ë¡œì§ (fallback)
                     for agent_score in eval_data.get("agents", []):
                        agent_id = agent_score.get("agent_id", "unknown")
                        total = agent_score.get("total", 0)
                        lines.append(f"- {agent_id}: {total:.1f}/100")

            lines.append("")

    # Agent í†µê³„
    lines.append("## Agent í†µê³„")
    lines.append("")
    lines.append("| Agent | ì„±ê³µ | ì‹¤íŒ¨ | ì„±ê³µë¥  |")
    lines.append("|-------|------|------|--------|")

    for agent_id, stats in agent_stats.items():
        total = stats["success"] + stats["failed"]
        rate = (stats["success"] / total * 100) if total > 0 else 0
        lines.append(f"| {agent_id} | {stats['success']} | {stats['failed']} | {rate:.1f}% |")

    lines.append("")
    lines.append("---")
    lines.append(f"ì´ ì‹œë‚˜ë¦¬ì˜¤: {total_scenarios}ê°œ")

    # ì €ì¥
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))


def main():
    """ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""
    import argparse

    parser = argparse.ArgumentParser(
        description="ISD Agent Benchmark ì‹¤í–‰"
    )
    parser.add_argument(
        "--install",
        action="store_true",
        help="Agent íŒ¨í‚¤ì§€ ì„¤ì¹˜",
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="Agent ì„¤ì¹˜ ìƒíƒœ í™•ì¸",
    )
    parser.add_argument(
        "--variant",
        "-t",
        type=str,
        default=None,
        help="ì‹¤í–‰í•  variant (ì‰¼í‘œ êµ¬ë¶„, ì˜ˆ: idld_aligned,context_variant). --datasetê³¼ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€",
    )
    parser.add_argument(
        "--dataset",
        "-d",
        type=str,
        choices=["train", "test"],
        default=None,
        help="ë°ì´í„°ì…‹ ì„ íƒ (train: í•™ìŠµìš©, test: í‰ê°€ìš©). --variantì™€ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€",
    )
    parser.add_argument(
        "--agents",
        "-a",
        type=str,
        default=None,
        help="ì‹¤í–‰í•  Agent (ì‰¼í‘œ êµ¬ë¶„)",
    )
    parser.add_argument(
        "--scenario",
        "-s",
        type=str,
        default=None,
        help="íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="ìƒì„¸ ì¶œë ¥",
    )
    # ë³‘ë ¬ ì‹¤í–‰ ì˜µì…˜ (ê¸°ë³¸ê°’: ìµœëŒ€ ë³‘ë ¬í™”)
    parser.add_argument(
        "--no-parallel",
        action="store_true",
        help="Agent ë³‘ë ¬ ì‹¤í–‰ ë¹„í™œì„±í™” (ê¸°ë³¸: ë³‘ë ¬ ì‹¤í–‰)",
    )
    parser.add_argument(
        "--max-workers",
        "-w",
        type=int,
        default=6,
        help="Agent ë™ì‹œ ì‹¤í–‰ ìˆ˜ (ê¸°ë³¸ê°’: 6, ëª¨ë“  ì—ì´ì „íŠ¸ ë™ì‹œ)",
    )
    parser.add_argument(
        "--no-scenario-parallel",
        action="store_true",
        help="ì‹œë‚˜ë¦¬ì˜¤ ë ˆë²¨ ë³‘ë ¬ ì‹¤í–‰ ë¹„í™œì„±í™” (ê¸°ë³¸: ë³‘ë ¬ ì‹¤í–‰)",
    )
    parser.add_argument(
        "--scenario-max-workers",
        type=int,
        default=8,
        help="Concurrent scenario execution count (default: 8)",
    )
    parser.add_argument(
        "--multi-judge",
        action="store_true",
        default=True,
        help="Use multi-judge evaluation with 5 LLMs (default: True)",
    )
    parser.add_argument(
        "--single-judge",
        action="store_true",
        help="Use single-judge evaluation (disable multi-judge)",
    )
    parser.add_argument(
        "--rate-limit",
        "-r",
        type=str,
        choices=["conservative", "moderate", "aggressive", "turbo"],
        default="conservative",
        help="Rate limit mode: conservative (2x2=4), moderate (3x4=12), aggressive (6x8=48), turbo (6x16=96). Default: conservative",
    )

    args = parser.parse_args()

    # Rate limit ëª¨ë“œì— ë”°ë¥¸ ì„¤ì • ì¡°ì •
    rate_limit_configs = {
        "conservative": {"max_workers": 2, "scenario_max_workers": 2, "delay": 2.0},
        "moderate": {"max_workers": 3, "scenario_max_workers": 4, "delay": 0.5},
        "aggressive": {"max_workers": 6, "scenario_max_workers": 8, "delay": 0.1},
        "turbo": {"max_workers": 6, "scenario_max_workers": 16, "delay": 0.0},
    }
    rate_config = rate_limit_configs[args.rate_limit]

    # ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ì§€ ì•Šì•˜ìœ¼ë©´ rate_limit ëª¨ë“œ ê°’ ì‚¬ìš©
    if args.max_workers == 6:  # default value
        args.max_workers = rate_config["max_workers"]
    if args.scenario_max_workers == 8:  # default value
        args.scenario_max_workers = rate_config["scenario_max_workers"]

    # ì „ì—­ ë”œë ˆì´ ì„¤ì •
    os.environ["BENCHMARK_DELAY"] = str(rate_config["delay"])

    # ì„¤ì¹˜
    if args.install:
        success = install_agents()
        sys.exit(0 if success else 1)

    # ì„¤ì¹˜ í™•ì¸
    if args.check:
        print("\nAgent ì„¤ì¹˜ ìƒíƒœ:")
        print("-" * 40)
        status = check_agents_installed()
        for agent, installed in status.items():
            icon = "âœ“" if installed else "âœ—"
            print(f"  {icon} {agent}")

        all_installed = all(status.values())
        if not all_installed:
            print("\nì¼ë¶€ Agentê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("--install ì˜µì…˜ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        sys.exit(0 if all_installed else 1)

    # ëª¨ë“ˆ import ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    status = check_agents_installed()
    if not all(status.values()):
        missing = [agent for agent, installed in status.items() if not installed]
        print(f"\nâš ï¸  Agent ëª¨ë“ˆ import ì‹¤íŒ¨: {', '.join(missing)}")
        print("sys.path ë˜ëŠ” ì˜ì¡´ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    # ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
    if args.scenario:
        scenario_path = Path(args.scenario)
        if not scenario_path.exists():
            print(f"ì˜¤ë¥˜: ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scenario_path}")
            sys.exit(1)

        output_dir = RESULTS_DIR / f"single_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        agents = args.agents.split(",") if args.agents else None

        # Determine multi-judge setting
        use_multi_judge = args.multi_judge and not args.single_judge

        result = run_single_benchmark(
            scenario_path=scenario_path,
            output_dir=output_dir,
            agents=agents,
            verbose=args.verbose,
            parallel=not args.no_parallel,
            max_workers=args.max_workers,
            multi_judge=use_multi_judge,
        )

        print(f"\nResults saved: {output_dir}")
        sys.exit(0)

    # --datasetê³¼ --variant ë™ì‹œ ì‚¬ìš© ë°©ì§€
    if args.dataset and args.variant:
        print("ì˜¤ë¥˜: --datasetê³¼ --variantëŠ” ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("  --dataset: train/test ë°ì´í„°ì…‹ ëª¨ë“œ (í‰ê°€ìš©)")
        print("  --variant: ê¸°ì¡´ variant ëª¨ë“œ (idld_aligned, context_variant)")
        sys.exit(1)

    # ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    variants = args.variant.split(",") if args.variant else None
    agents = args.agents.split(",") if args.agents else None

    # Determine multi-judge setting
    use_multi_judge = args.multi_judge and not args.single_judge

    results = run_full_benchmark(
        variants=variants,
        agents=agents,
        verbose=args.verbose,
        parallel=not args.no_parallel,
        max_workers=args.max_workers,
        scenario_parallel=not args.no_scenario_parallel,
        scenario_max_workers=args.scenario_max_workers,
        dataset=args.dataset,
        multi_judge=use_multi_judge,
    )

    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    timestamp = results["timestamp"]
    output_dir = results.get("output_dir", RESULTS_DIR / f"benchmark_{timestamp}")
    summary_path = Path(output_dir) / "SUMMARY.md"
    generate_summary_report(results, summary_path)
    print(f"ìš”ì•½ ë¦¬í¬íŠ¸: {summary_path}")


if __name__ == "__main__":
    main()
